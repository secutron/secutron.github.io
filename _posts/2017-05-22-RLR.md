---
title: Regularized Linear Regression
category: Machine Learning
tag: Linear Regression
---

이번 글에서는 회귀계수들에 제약을 가해 **일반화(generalization)** 성능을 높이는 기법인 **Regularized Linear Regression**에 대해 살펴보도록 하겠습니다. 이번 글 역시 고려대 김성범 교수님, 같은 대학의 강필성 교수님 강의, 미국 스탠포드 대학의 CS231n 강의 노트 일부를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 정규화의 목적

**정규화(regularization)**란 회귀계수가 가질 수 있는 값에 제약조건을 부여하는 방법입니다. 미래데이터에 대한 오차의 기대값은 모델의 **Bias**와 **variance**로 분해할 수 있는데요. 정규화는 variance를 감소시켜 일반화 성능을 높이는 기법입니다. 물론 이 과정에서 bias가 증가할 수 있기는 하지만요. 정규화의 이론적 배경인 Bias-Variance Decomposition에 대해 살펴보시려면 [이곳](https://ratsgo.github.io/machine%20learning/2017/05/19/biasvar/)을 참고하시기 바랍니다.

정규화의 결과를 직관적으로 나타낸 그림은 아래와 같습니다. 하단좌측 그림은 학습데이터를 정말 잘 맞추고 있지만, 미래 데이터가 조금만 바뀌어도 예측값이 들쭉날쭉할 수 있습니다. 반면 우측 그림은 가장 강한 수준의 정규화를 수행한 결과인데요. 학습데이터에 대한 설명력을 다소 포기하는 대신 미래 데이터 변화에 상대적으로 안정적인 결과를 냅니다. 

<a href="http://imgur.com/edAE2WU"><img src="http://i.imgur.com/edAE2WU.png" width="400px" title="source: imgur.com" /></a>



## 일반 선형회귀 모델

정규화는 **일반 선형회귀 모델(Ordinary Linear Regression Model)**에서 도출된 회귀계수들에 제약을 가하는 방법론입니다. 일반 선형회귀 모델은 종속변수($y$)의 실제값과 모델의 예측값 사이의 **평균제곱오차(Mean Square Error)**를 최소화하는 회귀계수들의 집합을 가리킵니다. 이러한 회귀계수를 뽑는 데 쓰는 기법을 **최소자승법(Least Squares Method)**라고 합니다.

우리가 가진 학습데이터의 독립변수가 $k$개, 관측치가 $n$개라고 칩시다. 그러면 이로부터 도출할 수 있는 일반 선형회귀 모델을 행렬 형태로 나타내면 다음과 같습니다. 아래 행렬에서 1은 회귀모델의 상수항에 대응하는 값입니다.


$$
\begin{bmatrix} \hat{ y }_{ 1 } \\ \hat{ y }_{ 2 } \\ ... \\\hat { y }_{ n } \end{bmatrix}=\begin{bmatrix} 1 & { x }_{ 11 } & ... & { x }_{ 1k } \\ 1 & { x }_{ 21 } & ... & { x }_{ 2k } \\ ... & ... & ... & ... \\ 1 & { x }_{ n1 } & ... & { x }_{ nk } \end{bmatrix}\begin{bmatrix} { \beta  }_{ 0 } \\ { \beta  }_{ 1 } \\ ... \\ { \beta  }_{ k } \end{bmatrix}
$$


이를 행렬식으로 간단히 나타내면 다음과 같습니다. 독립변수로 구성된 행렬 $X$, 종속변수로 구성된 벡터 $y$는 우리가 이미 가지고 있는 학습데이터이고 벡터 y-hat은 모델이 예측한 값입니다. 회귀계수 벡터 $β$가 모델 구축 결과물입니다.


$$
\hat { y } =X \beta  
$$


최소자승법은 실제값과 예측값의 MSE를 최소화하도록 합니다. 아래와 같이 쓸 수 있습니다.


$$
\begin{align*}
{ \hat { \beta  }^{ LS } } &=\min _{ \beta  }{ { (y-\hat { y } ) }^{ T }(y-\hat { y } ) } \\ &=\min _{ \beta  }{ { (y-X\beta ) }^{ T }(y-X\beta ) } 
\end{align*}
$$


최소자승법의 해인 회귀계수 벡터 $β$는 위 식을 $β$로 미분한 식을 0으로 놓고 풀면 다음과 같이 명시적으로 구할 수 있습니다.


$$
{ \hat { \beta  }^{ LS } }   ={ ({ X }^{ T }X) }^{ -1 }{ X }^{ T }y
$$


이렇게 구한 $β$는 bias가 없는 추정량 가운데 variance가 가장 작다고 합니다. 이름하여 **Best Linear Unbiased Estimator(BLUE)**입니다. 앞으로 설명해드릴 정규화 기법들은 bias를 소폭 허용(희생)하면서 variance를 줄이는 방법론이라고 할 수 있겠습니다.





## 릿지회귀

**릿지 회귀(Ridge Regression)**란 평균제곱오차를 최소화하면서 회귀계수 벡터 $β$의 $L_2$ norm을 제한하는 기법입니다. 선형회귀 모델의 목적식(MSE 최소화)과 회귀계수들에 대한 제약식을 함께 쓰면 아래와 같습니다. 여기에서 $λ$는 제약을 얼마나 강하게 걸지 결정해주는 값으로 사용자가 지정하는 하이퍼파라메터입니다.


$$
{  { \hat{ \beta  }^{ Ridge } }  } =\arg\min _{ \beta  }{ \left\{ { (y-X\beta ) }^{ T }(y-X\beta )+\lambda { \beta  }^{ T }\beta  \right\}  }
$$


릿지회귀의 해인 회귀계수 벡터 $β$는 위 식을 $β$로 미분한 식을 0으로 놓고 풀면 다음과 같이 명시적으로 구할 수 있습니다.


$$
{  { \hat{ \beta  }^{ Ridge } }  } ={ ({ X }^{ T }X+\lambda I) }^{ -1 }{ X }^{ T }y
$$


평균제곱오차뿐 아니라 $β$의 $L_2$ norm 또한 최소화하는 것이 릿지 회귀의 목적입니다. 우선 아래 예시 표를 볼까요?

<a href="http://imgur.com/8qnLAnf"><img src="http://i.imgur.com/8qnLAnf.png" width="400px" title="source: imgur.com" /></a>

MSE 기준으로는 벡터 $β$의 첫번째 요소 $β_1$과 두번째 요소 $β_2$가 각각 4, 5여야 최소입니다. 일반 선형회귀 모델이었다면 (4,5)가 회귀계수로 결정됐을 겁니다. 

하지만 여기에 $β_1^2+β_2^2$이 30 이하여야 한다는 제약을 가해 봅시다. 그러면 표 상단 세 가지 경우의 수는 고려에서 제외됩니다. 제약을 만족하는 나머지 세 개 가운데 MSE가 최소인 (2,4)를 회귀계수로 결정하는 것이 릿지 회귀의 방식입니다.



## 릿지회귀의 기하학적 이해

우리가 찾아야 하는 최적 회귀계수 벡터 $β$를 [$β_1, β_2$]라고 두겠습니다. 평균제곱오차를 식으로 쓰면 다음과 같습니다. (아래 식의 모든 요소는 스칼라값입니다)


$$
\begin{align*}
MSE({ \beta  }_{ 1 },{ \beta  }_{ 2 })=&\sum _{ i=1 }^{ n }{ { ({ y }_{ i }-{ \beta  }_{ 1 }{ x }_{ i1 }-{ \beta  }_{ 2 }{ x }_{ i2 }) }^{ 2 } } \\ =&\left( \sum _{ i=1 }^{ n }{ { x }_{ i1 }^{ 2 } }  \right) { \beta  }_{ 1 }^{ 2 }+\left( \sum _{ i=1 }^{ n }{ { x }_{ i2 }^{ 2 } }  \right) { \beta  }_{ 2 }^{ 2 }+2\left( \sum _{ i=1 }^{ n }{ { x }_{ i1 }{ x }_{ i2 } }  \right) { \beta  }_{ 1 }{ \beta  }_{ 2 }\\&-2\left( \sum _{ i=1 }^{ n }{ { y }_{ i }{ x }_{ i1 } }  \right) { \beta  }_{ 1 }-2\left( \sum _{ i=1 }^{ n }{ { y }_{ i }{ x }_{ i2 } }  \right) { \beta  }_{ 2 }+\sum _{ i=1 }^{ n }{ { y }_{ i }^{ 2 } }
\end{align*}
$$


위 식을 자세히 보시면 MSE는 $Aβ_1^2+Bβ_1β_2+Cβ_2^2+Dβ_1+Eβ_2+F$ 형태의 **원추곡선(conic equation)**이 됩니다. 타원, 쌍곡선, 원, 포물선은 원추곡선의 특수한 경우에 해당하는데요. 판별식 $B^2-4AC$이 0보다 작으면 타원 형태가 된다고 합니다. 위 식에서 판별식을 계산해 보면 **코시-슈바르츠 부등식** 조건에 의해 0 이하가 됩니다. 

따라서 MSE가 같은 [$β_1, β_2$]의 자취를 그려보면 **타원** 모양이 된다는 겁니다. 바로 아래 그림처럼요.

<a href="http://imgur.com/RBxwo0D"><img src="http://i.imgur.com/RBxwo0D.png" width="700px" title="source: imgur.com" /></a>

좌측상단 그림에서 타원 모양의 녹색 실선은 MSE가 동일한 [$β_1, β_2$]의 자취입니다. $β^{LS}$라고 표시된 검정색 점은 일반 선형회귀 모델의 결과로, MSE가 최소가 되는 지점입니다. 여기에서 멀리 떨어져 있는 타원일 수록 MSE가 점점 커진다고 이해하면 좋을 것 같습니다.

좌측상단 그림에서 파란색 원은 $β$의 $L_2$ norm이 동일한 [$β_1, β_2$]의 자취입니다. 원의 반지름이 작아질 수록 $L_2$ norm이 감소하고, 그만큼 제약이 커진다고 이해하면 좋을 것 같습니다. 다시 말해 하이퍼파라메터 $λ$가 클수록 $β$의 $L_2$ norm이 줄어듭니다. 

우리가 특정 $λ$을 지정해 $β_1^2+β_2^2$이 $t_3$ 이하가 되도록 제약을 가했다고 가정해 봅시다. 그러면 릿지 회귀 기법은 이러한 제약을 만족하면서도 MSE가 최소인 지점에 해당하는 [$β_1, β_2$]를 찾게 됩니다. 위 예시 기준으로는 $β^{LS}$라고 표시된 검정색 점과 가장 가까운 점이 릿지 회귀의 결과가 될 겁니다.

하이퍼파라메터 $λ$를 0으로 두면 릿지 회귀의 제약식($λβ^Tβ$)이 사라지기 때문에 일반 선형회귀 모델의 결과와 동일한 회귀계수들을 얻을 수 있습니다. 하지만 $λ$가 커질수록(=$t$가 작아질수록) 회귀계수들에 가해지는 제약이 커져서 계수들의 값이 점점 줄어드는 모습을 우측상단 그래프에서 확인할 수 있습니다.

마지막으로 릿지회귀에서의 ridge는 산등성이라는 뜻을 가졌는데요. 위 그림에서처럼 MSE와 제약식이 가지는 자취가 산등성이 모양을 지녀서 이런 이름이 붙은 것 아닌가 생각합니다.



## 라쏘회귀

**라쏘회귀(Least Absolute Shrinkage and Selection Operator)**의 목적식과 제약식을 한번에 쓰면 다음과 같습니다. 릿지회귀와 동일하지만 $L_1$ norm을 제약한다는 점이 다릅니다.


$$
{ { \hat { \beta  } ^{ Lasso } } }=\min _{ \beta  }{ \left\{ { (y-X\beta ) }^{ T }(y-X\beta )+\lambda { \left\| \beta  \right\|  }_{ 1 } \right\}  }
$$


요소 개수가 $p$개인 회귀계수 벡터 $β$의 $L_1$ norm은 각 요소에 절대값을 취한 뒤 모두 더해 구합니다. 아래와 같습니다.


$$
{ \left\| \beta  \right\|  }_{ 1 }=\sum _{ i=1 }^{ p }{ \left| { \beta  }_{ i } \right|  }
$$


$L_1$ norm은 미분이 불가능하기 때문에 라쏘회귀의 경우 해를 단박에 구할 수 없습니다. 이 때문에 numerial 기법들이 다양하게 제시됐습니다.



## 라쏘회귀의 기하학적 이해

우리가 찾아야 하는 최적 회귀계수 벡터 $β$를 [$β_1, β_2$]라고 두고, 이를 그림으로 나타내면 다음과 같습니다.



<a href="http://imgur.com/xsa2fNQ"><img src="http://i.imgur.com/xsa2fNQ.png" width="600px" title="source: imgur.com" /></a>



MSE가 동일한 [$β_1, β_2$]의 자취는 릿지회귀 때와 마찬가지로 타원입니다. 제약식은 $L_1$ norm(절대값)을 썼기 때문에 $L_2$ norm이 동일한 [$β_1, β_2$]의 자취는 마름모 꼴이 됩니다. 

파란색 마름모 꼴의 제약 범위 내에서 MSE가 최소인 점은 $β_2$축 위의 검정색 점입니다. 이 점은 바로 $β_1=0$인 지점인데요. $β_1$이 0이라는 이야기는 그에 대응하는 독립변수 $x_1$이 예측에 중요하지 않다는 말과 같습니다. 

이처럼 라쏘회귀는 예측에 중요하지 않은 변수의 회귀계수를 감소시킴으로써 **변수선택(Feature Selection)**하는 효과를 낸다고 합니다.



## 엘라스틱넷

**엘라스틱넷(Elastic Net)**은 제약식에 $L_1, L_2$ norm 모두 쓰는 기법입니다. 목적식과 제약식을 한번에 쓰면 다음과 같습니다.


$$
{ { \hat { \beta  } ^{ enet } } }=\min _{ \beta  }{ \left\{ { (y-X\beta ) }^{ T }(y-X\beta )+{ \lambda  }_{ 1 }{ { \left\| \beta  \right\|  }_{ 1 }+\lambda  }_{ 2 }{ \beta  }^{ T }\beta  \right\}  } 
$$




## 릿지회귀 vs 라쏘회귀 vs 엘라스틱넷

세 가지 방법을 비교한 표는 다음과 같습니다.

|    구분    |        릿지회귀         |       라쏘회귀        |         엘라스틱넷         |
| :------: | :-----------------: | :---------------: | :-------------------: |
|   제약식    |     $L_2$ norm      |    $L_1$ norm     |   $L_1$+$L_2$ norm    |
|   변수선택   |         불가능         |        가능         |          가능           |
| solution |     closed form     |      명시해 없음       |        명시해 없음         |
|    장점    | 변수간 상관관계가 높아도 좋은 성능 | 변수간 상관관계가 높으면 성능↓ |   변수간 상관관계를 반영한 정규화   |
|    특징    | 크기가 큰 변수를 우선적으로 줄임  | 비중요 변수를 우선적으로 줄임  | 상관관계가 큰 변수를 동시에 선택/배제 |

세 기법의 제약식의 자취를 그림으로 나타내면 아래와 같습니다. (녹색 점선=릿지회귀, 검정색 점선=라쏘회귀, 파란색 도형=엘라스틱넷)

<a href="http://imgur.com/7rPnTjz"><img src="http://i.imgur.com/7rPnTjz.png" width="400px" title="source: imgur.com" /></a>



## 기타 정규화 기법들

인접한 변수들을 동시에 선택하는 **Fused Lasso**, 사용자가 정의한 그룹 단위로 변수를 선택하는 **Group Lasso**, 사용자가 정의한 그래프의 연결 관계에 따라 변수를 선택하는 **Graph-Constrained Regularization** 등이 있습니다.





## 딥러닝과의 연계

이미지가 주어졌을 때 해당 이미지가 고양이인지 개인지 배인지를 맞추는 문제를 푼다고 칩시다. 다음과 같이 다범주 선형회귀 모델을 만들 수 있습니다.



<a href="https://imgur.com/Lm6DF5y"><img src="https://i.imgur.com/Lm6DF5y.png" title="source: imgur.com" /></a>



여기서 하나 상상해 봅시다. 만약 입력데이터 $x$가 [1, 1, 1, 1]이고 cat score를 만드는 데 쓰이는 가중치 벡터($W$의 첫 행벡터) $w_1$이 [1, 0, 0, 0]이라 칩시다. dog score를 만드는 가중치 벡터 $w_2$는 [0.25, 0.25, 0.25, 0.25]라 칩시다. 이렇게 되면 $w_1x=w_2x=1$이 되어 같은 결과를 냅니다. 다만 차이가 있습니다. $w_1$은 데이터의 특정 영역만 보고 dog인지 판단한다면, $w_2$는 데이터를 전체적으로 보고 판단한다는 겁니다.

이번엔 ship score를 내는 $w_3$가 [2, 2, 2, 2]라고 칩시다. $w_3x=8$이 되어 최종적으로 이 그림이 ship으로 분류되게 됩니다. 그런데 이렇게 되면 ship이라는 클래스가 cat이나 dog보다 더 큰 영향력을 발휘하게 되어 결과적으로 미지의 데이터에 대한 일반화 성능이 떨어지게 됩니다(어떤 그림이 들어와도 ship이라고 분류). 손실 최소화와 동시에 정규화(regularization)를 하는 이유입니다. 위 예시에서 $w_1$의 L2 norm은 1, $w_2$는 0.25, $w_3$은 16입니다. 