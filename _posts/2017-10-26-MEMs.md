---
title: 최대엔트로피모델(Maximum Entropy Models)
category: Machine Learning
tag: MEMs
---

이번 글에선 **최대엔트로피모델(Maximum Entropy Models, MEMs)**을 다루어 보도록 하겠습니다. 이 글은 고려대 강필성 교수님 강의와 역시 같은 대학의 정순영 교수님 강의, 서울대 언어학과 신효필 교수님 저서, 위키피디아 등을 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.





## concepts

자연언어처리 분야에서는 **다항로지스틱 회귀(multinominal logistic regression)**를 최대엔트로피모델이라 부릅니다. 다항로지스틱 회귀에 대한 자세한 내용은 [이곳](https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/)을 참고하시면 좋을 것 같습니다. 단어 $x$가 주어졌을 때 범주(예 : 품사) $c$가 나타날 확률은 다음과 같습니다. 


$$
P(c|x)=\frac { exp\left( { \overrightarrow { { w }_{ c } }  }^{ T }\overrightarrow { f }  \right)  }{ \sum _{ c'\in C }^{  }{ { exp( }{ \overrightarrow { { w }_{ c' } }  }^{ T }\overrightarrow { f } ) }  }
$$


위 식에서 벡터 $f$는 단어 $x$에 해당하는 자질(feature)들의 모음입니다. 예컨대 자질 벡터의 첫번째 요소는 '직전 단어가 명사이면 1, 그렇지 않으면 0', 두번째 요소는 '현재 단어가 동사이면 1, 아니면 0'... 이런 식으로 $f$의 요소값들을 구성합니다.

$w_c$는 자질벡터 $f$만큼의 차원수를 갖는 가중치 벡터입니다. 분류해야 할 범주의 개수만큼 필요합니다. 데이터가 주어지면 그래디언트 어센트 등 기법으로 우도를 최대화하는 방향으로 학습됩니다. 자질벡터의 특정 요소를 집중 반영하거나, 특정 범주에만 높은 확률을 내지 않도록 정규화(regularization)도 수행해 줍니다. 로지스틱 회귀의 학습에 대해서는 [이곳](https://ratsgo.github.io/machine%20learning/2017/07/02/logistic/), 정규화에 대해서는 [이곳](https://ratsgo.github.io/machine%20learning/2017/05/22/RLR/)을 참고하시면 좋을 것 같습니다.





## feature vector

최대엔트로피모델의 핵심은 자질벡터입니다. 연구자의 언어학적 사전지식을 매우 유연하게 반영할 수 있기 때문에, 초기값 설정에만 개입할 수 있는 [은닉마코프모델](https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/) 등과 비교해 최대엔트로피모델의 강점이라고 말할 수 있겠습니다. (물론 자질 요소들을 연구자가 일일이 수작업으로 지정해주어야 하기 때문에 최대 약점으로 꼽히기도 합니다)

예컨대 다음과 같은 문장이 주어졌고, 최대엔트로피모델은 이미 4개 단어에 대한 품사 분류를 마쳤으며, 이번에는 'race'를 예측해야 한다고 가정해 보겠습니다. (최대엔트로피모델은 이전 예측결과만 분류에 활용)

> Secretariat**/NNP** is**/BEZ** expected**/VBZ** to**/TO** race tomorrow

현재 단어 $x$는 'race'입니다. $x$가 가질 수 있는 범주는 동사, 명사 두 개뿐이라고 연구자가 판단했다고 가정해 보겠습니다. 즉 $C$={VB, NN}입니다. $c$와 $x$가 주어졌을 때 자질벡터의 $i$번째 요소값을 만들어내는 함수 $f_i(c,x)$를 연구자의 언어학적 사전지식을 반영해 다음과 같이 정의했다고 치겠습니다.



<a href="https://imgur.com/9kqNqZy"><img src="https://i.imgur.com/9kqNqZy.png" width="400px" title="source: imgur.com" /></a>



자질함수는 6개, 'race'가 가질 수 있는 범주는 2개라고 정의했기 때문에 자질벡터 $f(c,x)$는 6차원이며, 동사 명사 각각 하나씩 총 2개 만들어 집니다. 다음과 같습니다.

|   notation   |     value     |
| :----------: | :-----------: |
| $f(VB,race)$ | [0,1,0,1,1,0] |
| $f(NN,race)$ | [1,0,0,0,0,1] |





## prediction

다항로지스틱 모델의 파라메터($w$)는 이미 학습이 끝났다고 가정하고 자질벡터를 입력으로 예측이 어떻게 이뤄지는지 살펴보겠습니다. 먼저 동사 먼저 보겠습니다. 아래 표에서 자질벡터 요소값과 그에 해당하는 가중치를 곱합니다.

|     차원수      | $f_1$ | $f_2$ | $f_3$ | $f_4$ | $f_5$ | $f_6$ |
| :----------: | :---: | :---: | :---: | :---: | :---: | :---: |
| $f(VB,race)$ |   0   |   1   |   0   |   1   |   1   |   0   |
|   $w_{VB}$   |       |  .8   |       |  .01  |  .1   |       |

다음은 명사입니다. 위와 동일한 과정을 거칩니다.

|     차원수      | $f_1$ | $f_2$ | $f_3$ | $f_4$ | $f_5$ | $f_6$ |
| :----------: | :---: | :---: | :---: | :---: | :---: | :---: |
| $f(NN,race)$ |   1   |   0   |   0   |   0   |   0   |   1   |
|   $w_{NN}$   |  .8   |       |       |       |       | -1.3  |

이제 'race'가 각 범주에 속할 확률을 계산해 보겠습니다. 다음과 같습니다. 따라서 race는 동사로 분류합니다.



$$
P\left( VB|race \right) =\frac { { e }^{ .8+.01+.1 } }{ { e }^{ .8+.01+.1 }+{ e }^{ .8-1.3 } } =0.8\\ P\left( NN|race \right) =\frac { { e }^{ .8-1.3 } }{ { e }^{ .8+.01+.1 }+{ e }^{ .8-1.3 } } =0.2
$$






## Why Maximum Entropy?

최대엔트로피모델은 그럼 왜 이런 이름이 붙은 걸까요? 최대엔트로피모델을 구축하려면 먼저 해당 단어가 가질 수 있는 품사를 가려내고, 자질도 정의해야 하는데요. 예컨대 'zzfish'라는 단어의 품사를 예측한다고 가정해 보겠습니다. 

연구자가 난생 처음 보는 단어라서 품사 후보들을 가려내기 어렵고 자질과 관련해 'zzfish'에 대한 어떤 가정도 할 수 없습니다. 이 경우 해당 언어(영어)에 존재하는 모든 품사 종류(예컨대 45개)가 후보에 오를 것입니다. 모델 예측 결과가 다음 표와 같이 같은 확률을 가진 분포(equiprobable distribution)가 된다면 이상적입니다.

|  NN  |  JJ  | NNS  |  VB  | NNP  | ...  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 1/45 | 1/45 | 1/45 | 1/45 | 1/45 | ...  |

그런데 연구자가 말뭉치를 살펴보니 'fish'가 포함된 단어는 그 품사의 종류가 명사(NN), 형용사(JJ), 복수형 명사(NNS), 동사(VB)라는 사실을 알게 됐다고 가정해 보겠습니다. 연구자가 품사 후보를 이같이 정한다면 'zzfish'의 품사는 이 네 가지 중 하나가 될 것입니다. 하지만 추가 가정이 없기 때문에 모델은 넷 사이에서는 같은 확률로 예측하기를 기대합니다.

|  NN  |  JJ  | NNS  |  VB  | NNP  | ...  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 1/4  | 1/4  | 1/4  | 1/4  |  0   | ...  |

말뭉치를 보니 'zzfish' 10개 중 8개는 명사라고 칩시다. 이걸 보고 연구자는 현재 예측할 단어 $x$가 'zzfish'이고, $c$가 명사나 복수형 명사이면 1로 하는 자질함수를 정의했습니다. 따라서 우리는 품사 분포는 다음과 같이 바뀌기를 기대합니다.

|  NN  |  JJ  | NNS  |  VB  | NNP  | ...  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 4/10 | 1/10 | 4/10 | 1/10 |  0   | ...  |

우리가 가진 말뭉치에서 'zzfish'에 대한 정보를 최대한 다 뽑아 냈다고 칩시다. 그런데 연구자가 영어 전체 말뭉치를 살펴보니 동사는 평균적으로 1/20의 확률로 나타났다는 사실을 알게 됐습니다. 그러면 분포가 또 바뀝니다.

|  NN  |  JJ  | NNS  |  VB  | NNP  | ...  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 4/10 | 3/20 | 4/10 | 1/20 |  0   | ...  |

[엔트로피](https://ratsgo.github.io/statistics/2017/09/22/information/)란 불확실성 정도를 나타내는 지표입니다. 위 네 개 케이스에 대해 엔트로피(밑이 2인 log로 계산)를 구하면 각각 5.491, 2, 1.722, 1.6842입니다. 균등한 분포일 때 엔트로피가 제일 높고, 분포가 불균등해질 수록 엔트로피가 점점 낮아지는 것을 확인할 수 있습니다.

그런데 위 과정을 천천히 살펴보면 연구자가 말뭉치를 관찰하면서 알게 된 사실이 범주 후보나 자질 함수의 형태로 추가되면서 점점 불균등한 분포를 띄게 됐습니다. 다시 말해 추가적인 정보나 전제가 없다면 최대엔트로피모델은 균등한 분포를 전제하고, 이는 최대 엔트로피를 지니게 된다는 이야기입니다.