---
title: 정보이론 기초
category: Statistics
tag: Information Theory
---

이번 글에서는 정보이론(Information Theory)의 기본 개념들을 살펴보도록 하겠습니다. 이 글은 Ian Goodfellow 등이 집필한 Deep Learning Book과 위키피디아를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 개요

정보이론은 시그널에 존재하는 정보의 양을 측정하는 응용수학의 한 갈래입니다. 정보이론은 무선전송을 통해 알파벳으로 된 메세지를 보내려는 연구에서 시작되었습니다. 이 때 정보이론은 최적의 코드를 디자인하고, 메세지의 기대 길이(expected length)를 계산하는 데 도움이 됩니다. 머신러닝에서는 해당 확률분포의 특성을 알아내거나 확률분포 간 유사성을 정량화하는 데 쓰입니다.

정보이론의 핵심 아이디어는 잘 일어나지 않는 사건(unlikely event)은 자주 발생하는 사건보다 정보량이 많다(informative)는 것입니다. 예컨대 '아침에 해가 뜬다'는 메세지로 보낼 필요가 없을 정도로 정보 가치가 없습니다. 그러나 '오늘 아침에 일식이 있었다'는 메세지는 정보량 측면에서 매우 중요한 사건입니다. 이 아이디어를 공식화해서 표현하면 다음과 같습니다.

- 자주 발생하는 사건은 낮은 정보량을 가진다. 발생이 보장된 사건은 그 내용에 상관없이 전혀 정보가 없다는 걸 뜻한다.
- 덜 자주 발생하는 사건은 더 높은 정보량을 가진다.
- 독립사건(independent event)은 추가적인 정보량(additive information)을 가진다. 예컨대 동전을 던져 앞면이 두번 나오는 사건에 대한 정보량은 동전을 던져 앞면이 한번 나오는 정보량의 두 배이다.





## 섀넌 엔트로피

위 세 가지 조건을 만족하는 함수는 발생 가능한 사건이나 메세지의 확률분포에 음의 로그를 취한 수식입니다. 확률변수 $X$의 값이 $x$인 사건의 정보량은 아래와 같습니다.


$$
I\left( x \right) =-\log { P(x) }
$$


예컨대 동전을 던져 앞면이 나오는 사건과 주사위를 던져 눈이 1이 나오는 사건, 두 개의 정보량을 비교해보겠습니다. 전자의 정보량은 $-\log_{2}{0.5}=1$, 후자는 $-\log_{2}{1/6}=2.5849$가 됩니다. 다시 말해 주사위 눈이 1이 나올 사건은 동전의 앞면이 나오는 사건보다 덜 자주 발생하므로 더 높은 정보량을 갖는다는 의미입니다. 

위 식에서 밑이 2인 경우 정보량의 단위를 섀년(shannon) 또는 비트(bit)라고 합니다. 자연상수(exp)를 밑으로 할 경우 내트(nat)라고 부릅니다. 머신러닝에서는 대개 밑을 자연상수로 사용합니다. **섀넌 엔트로피(Shannon entropy)**는 모든 사건 정보량의 기대값을 뜻합니다. 전체 사건의 확률분포의 불확실성의 양을 나타낼 때 씁니다. 어떤 확률분포 $P$에 대한 섀넌 엔트로피 $H(P)$는 다음과 같습니다.


$$
H\left( P \right) =H\left( x\right) ={ E }_{ X\sim P }\left[ I\left( x \right)  \right] =-{ E }_{ X\sim P }\left[ \log { P(x) }  \right]
$$


앞면, 뒷면이 나올 확률이 동일한, 공평한 동전을 1번 던지면 섀년 엔트로피는 발생 가능한 모든 결과의 가지 수(2)에 밑이 2인 로그를 취한 것(=1비트)과 같습니다. 이를 식으로 풀어 쓰면 다음과 같습니다. 


$$
\begin{align*}
H\left( P \right)= H\left( x \right) &=-\sum _{ x }^{  }{ P({ x })\log { P({ x }) }  } \\ &=-\left( 0.5\times \log _{ 2 }{ 0.5 } +0.5\times \log _{ 2 }{ 0.5 }  \right) \\ &=-\log _{ 2 }{ 0.5 } \\ &=-(-1)
\end{align*}
$$




마찬가지로 2개 동전을 던지면 4가지 결과가 발생하고 섀년 엔트로피는 2비트가 됩니다. 다시 말해 서로 독립인 두 확률변수의 섀넌 엔트로피는 각 확률변수의 엔트로피 합과 같게 됩니다. 이를 그림으로 나타내면 다음과 같습니다.



<a href="https://imgur.com/PMVu70y"><img src="https://i.imgur.com/PMVu70y.jpg" width="400px" title="source: imgur.com" /></a>



사건의 분포가 결정적(deterministic)이라면 해당 확률분포의 불확실성 정도를 나타내는 엔트로피는 낮아집니다. 반대로 분포가 균등적(uniform)일 수록 엔트로피는 높아집니다. 동전 던지기 예를 다시 들면, 절대로 뒷면이 나오지 않는 동전을 던지는 건 아무런 정보를 가지지 않습니다. 항상 앞면이 나와서 불확실성이 전혀 없기 때문입니다. 하지만 공평한 동전을 던질 경우 엔트로피는 가장 높습니다. 결과값을 예상하기가 가장 어렵기 때문입니다. 



<a href="https://imgur.com/Pynf9sG"><img src="https://i.imgur.com/Pynf9sG.png" width="400px" title="source: imgur.com" /></a>



위 그래프는 동전을 한번 던졌을 때 섀넌 엔트로피의 변화를 나타낸 그림입니다. $x$축은 동전의 공정한 정도(1, 즉 앞면이 나올 확률)를 나타냅니다. 0.5라면 앞면, 뒷면이 나올 확률이 각각 동일한 공평한 동전이라는 뜻입니다. $y$축은 섀넌 엔트로피를 가리킵니다. 여기서는 공평한 동전을 사용할 경우가 가장 큰 엔트로피(1비트)를 나타내고 있습니다. 동전 던지기 결과값을 어딘가에 전송할 경우 공평한 동전을 쓸 때 최대 1비트가 필요함을 알 수 있습니다.





## KL Divergence

쿨백-라이블러 발산(Kullback-Leibler divergence, KLD)은 두 확률분포의 차이를 계산하는 데 사용하는 함수입니다. 딥러닝 모델을 만들 때 예로 들면 우리가 가지고 있는 데이터의 분포 $P(x)$와 모델이 추정한 데이터의 분포 $Q(x)$ 간에 차이를 KLD를 활용해 구할 수 있습니다. KLD의 식은 다음과 같이 정의됩니다.


$$
{ D }_{ KL }\left( P||Q \right) ={ E }_{ X\sim P }\left[ \log { \frac { P\left( x \right)  }{ Q(x) }  }  \right] ={ E }_{ X\sim P }\left[ \log { P(x) } -\log { Q(x) }  \right] 
$$


$P$와 $Q$가 동일한 확률분포일 경우 KLD는 정의에 따라 그 값이 0이 됩니다. 하지만 KLD는 비대칭(not symmetric)으로 $P$와 $Q$ 위치가 뒤바뀌면 KLD 값도 달라집니다. 따라서 KLD는 거리함수로는 사용할 수 없습니다.





## 크로스 엔트로피

KLD는 딥러닝 모델의 손실함수(loss function)로 자주 쓰이는 크로스 엔트로피(cross entropy)와 깊은 관련을 맺고 있습니다. $P$와 $Q$에 대한 크로스 엔트로피 $H(P,Q)$와 KLD와의 관계식은 다음과 같습니다.


$$
H\left( P,Q \right) =H\left( P \right) +{ D }_{ KL }\left( P||Q \right) 
$$


딥러닝 모델을 학습할 때 크로스 엔트로피를 최소화하는 방향으로 파라메터(가중치)들을 업데이트 합니다. 그런데 $Q$에 대해 크로스 엔트로피를 최소화한다는 것은 KLD를 최소화하는 것과 그 의미가 완전히 같습니다(equivalent). 왜냐하면 $P$는 우리가 가지고 있는 데이터의 분포를 가리키는데, $P$가 학습 과정에서 바뀌는 것이 아니기 때문입니다. 

어쨌든 크로스 엔트로피 최소화는 KLD 최소화와 같은 의미이며 우리가 가지고 있는 데이터의 분포 $P(x)$와 모델이 추정한 데이터의 분포 $Q(x)$ 간에 차이를 최소화한다는 정도로 이해하면 좋을 것 같습니다. 딥러닝 모델의 입력값으로 쓰이는 관측치는 이산변수(discrete variable)에 해당하므로 크로스 엔트로피 $H(P,Q)$의 식을 다시 쓰면 다음과 같습니다.



$$
H\left( P,Q \right) =-{ E }_{ X\sim P }\left[ \log { Q(x) }  \right] =-\sum _{ x }^{  }{ P({ x })\log { Q({ x }) }  }
$$



보시다시피 $H(P,Q)$는 $P$의 엔트로피인 $H(P)$와 유사한 꼴이나 로그 바깥에 곱해지는 확률이 $P(x)$이고, 로그 안에 들어가는 것이 $Q(x)$인 것을 확인할 수 있습니다. 엔트로피는 엔트로피이되 두 확률분포가 교차로 곱해진다는 의미로 크로스(cross) 엔트로피라는 이름이 붙은 것 같습니다.