---
title: unsupervised generative models 
category: generative model
tag: [discriminative, generative]
---

이번 글에서는 비지도학습(unsupervised learning) 기반의 **generative model**을 가우시안믹스처모델(Gaussian Mixture Model, GMM) 중심으로 살펴보도록 하겠습니다. 이 글은 전인수 서울대 박사과정이 2017년 12월에 진행한 패스트캠퍼스 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.





## generative model

*generative model*이란 데이터 $X$가 생성되는 과정을 두 개의 확률모형, 즉 $p(Y)$, $p(X$\|$Y)$으로 정의하고, 베이즈룰을 사용해 $p(Y$\|$X)$를 간접적으로 도출하는 모델을 가리킵니다. *generative model*은 레이블 정보가 있어도 되고, 없어도 구축할 수 있습니다. 이 글에서는 레이블 정보 없이 구축하는 비지도학습 기반의 *generative model*을 살펴보겠습니다.





## Gaussian Mixtrue Model

GMM은 [선형판별분석]()의 일반화된 버전입니다. GMM을 *generative modeling* 관점에서 분석하면 다음과 같습니다. 



$$
p\left( { z }^{ (i) } \right) \sim Multinomial(\phi )\\ p\left( { x }^{ (i) }|{ z }^{ (i) }=j \right) \sim N\left( { \mu  }_{ j },{ \Sigma  }_{ j } \right) 
$$


선형판별분석과 가장 큰 차이점은 레이블 $y$가 없는 비지도학습이라는 사실입니다. $y$ 대신 $z$가 등장했습니다. 그런데 $z$는 학습데이터로 주어지지 않은 잠재변수(latent variable)입니다. $z$는 [다항분포](https://ratsgo.github.io/statistics/2017/05/28/binomial/)를 따른다고 가정합니다. 이렇게 뽑힌 $z$가 $j$번째 id일 때 $x$가 나타날 확률은 $j$번째 평균과 분산을 모수로 갖는 정규분포를 따를 것이라 가정합니다.

GMM의 로그우도 함수는 다음과 같습니다. 단 여기에서 파라메터 $θ$는 (1)$z$와 관련된 다항분포 파라메터 $Φ$ (2)각 정규분포의 평균 $μ$ (3)정규분포의 분산 $Σ$ 세 가지를 가리킵니다. 



$$
\begin{align*}
l(\theta )=\sum _{ i }^{  }{ \log { \sum _{ { z }_{ i } }^{  }{ p\left( { x }^{ i },{ z }^{ i },\theta  \right)  }  }  } 
\end{align*}
$$


$z$가 학습데이터로 주어진 상황이라면, 우리가 구하고자 하는 각각의 파라메터들에 대해 위 로그우도 함수를 편미분한 결과가 0인 지점에서, 쉽게 파라메터를 추정할 수 있을 것입니다. 그러나 $z$는 잠재변수이기 때문에 위의 로그우도 함수를 최대화하는 파라메터를 단박에 구할 수 없습니다. 이 때문에 **EM 알고리즘**을 다음과 같이 적용합니다.

- **Expectation** : 로그우도 함수값의 하한(lower bound)을 구한다.
- **Maximization** : E-step에서 구한 로그우도 함수값을 최대화하는 파라메터를 찾는다.

그렇다면 로그우도 함수의 하한은 어떻게 구할까요? **Jensen's inequality**를 이용해 봅시다. 임의의 함수 $f$가 [볼록함수(convex function)](https://ratsgo.github.io/convex%20optimization/2017/12/26/convexfunction/)이고 $x$가 확률변수(random variable)이면 다음이 성립한다고 합니다.


$$
E\left[ f\left( x \right)  \right] \ge f\left( E\left[ x \right]  \right) 
$$
<a href="https://imgur.com/5HREVDw"><img src="https://i.imgur.com/5HREVDw.png" width="350px" title="source: imgur.com" /></a>



그런데 로그 함수($f$)는 오목함수(concave function)이므로 위의 부등식 방향이 반대로 적용될 겁니다. 또 여기에서 $z^i$에 대한 임의의 확률분포 $Q_i(z^i)$를 상정해 둡시다. Jensen's inequality와 $Q$를 활용해 GMM의 로그우도함수를 다음과 같이 다시 적을 수 있습니다.


$$
\begin{align*}
l(\theta )=&\sum _{ i }^{  }{ \log { \sum _{ { z }_{ i } }^{  }{ p\left( { x }^{ i },{ z }^{ i },\theta  \right)  }  }  } \\ =&\sum _{ i }^{  }{ \log { \sum _{ { z }_{ i } }^{  }{ { Q }_{ i }\left( { z }^{ i } \right) \frac { p\left( { x }^{ i },{ z }^{ i },\theta  \right)  }{ { Q }_{ i }\left( { z }^{ i } \right)  }  }  }  } \\ \ge& \sum _{ i }^{  }{ \sum _{ { z }_{ i } }^{  }{ { Q }_{ i }\left( { z }^{ i } \right) \log { \frac { p\left( { x }^{ i },{ z }^{ i },\theta  \right)  }{ { Q }_{ i }\left( { z }^{ i } \right)  }  }  }  } 
\end{align*}
$$


그런데 실제 로그우도는 $Q_i$와 상관이 없기 때문에 가급적 위 부등식이 '='에 가깝게 정할 수 있으면 좋을 겁니다. Jensen's inequality에서 등호는 $f(x)$가 선형(linear)일 때 성립한다고 합니다. 아울러 $Q_i$ 또한 확률분포이므로 그 합이 1이 되어야 합니다. 따라서 다음 두 가지 속성을 만족하도록 $Q_i$를 정하면 좋을 겁니다.


$$
\frac { p\left( { x }^{ i },{ z }^{ i },\theta  \right)  }{ { Q }_{ i }\left( { z }^{ i } \right)  } =c\leftrightarrow { Q }_{ i }\left( { z }^{ i } \right) \propto p\left( { x }^{ i },{ z }^{ i },\theta  \right) \\ \sum _{ z }^{  }{ { Q }_{ i }\left( { z } \right)  } =1
$$


그런데 $Q_i$를 $z$에 대한 사후확률(posterior)로 정하면 위 두 개 가정을 만족시킬 수 있다고 합니다. 다시 말해 $Q_i(z^i)=p(x^i,z^i,θ)/p(x^i,θ)=p(z^i$\|$x^i,θ)$로 둡니다. 요컨대 GMM의 EM 알고리즘은 고정된 $θ$ 하에서 $Q_i(z^i)$를 구하고, E-step에서 구한 $Q_i(z^i)$ 하에서 로그우도 함수를 최대화하는 $θ$를 구하는 과정을 반복합니다. 이와 관련해 고려대 강필성 교수님의 비즈니스어낼리틱스 강의노트를 참고용으로 올려둡니다.



<a href="https://imgur.com/bv462aT"><img src="https://i.imgur.com/bv462aT.png" title="source: imgur.com" /></a>





## Latent Dirichlet Allocation

일명 토픽모델링으로도 유명한 [잠재디리클레할당(Latent Dirichlet Allocation, LDA)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/) 또한  비지도학습 기반의 *generative model*입니다. LDA가 가정하는 문서생성과정은 다음과 같습니다.



- Draw each per-corpus topic distributions $ϕ_k$~$Dir(β)$ for $i∈${$1,2,...K$}
- For each document, Draw per-document topic proportions $θ_d$~$Dir(α)$
- For each document and each word, Draw per-word topic assignment $z_{d,n}$~$Multi(θ_d)$
- For each document and each word, Draw observed word $w_{d,n}$~$Multi(ϕ_{z_{d,n},n})$



LDA에서는 파라메터 추정 과정에서 [켤레사전분포](https://ratsgo.github.io/statistics/2017/05/28/binomial/)를 가정하고, [깁스 샘플링](https://ratsgo.github.io/statistics/2017/05/31/gibbs/)을 사용합니다.