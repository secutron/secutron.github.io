---
title: Branching Entropy
category: From frequency to semantics
tag: tokenize
---
이번 글에서는 말뭉치에서 단어를 추출하는 기법 가운데 하나인 **Branching Entropy(이하 BE)**에 대해 살펴보도록 하겠습니다. BE는 Jin&Tanaka(2006)이 제안한 모델인데요, 이 글은 김현중 서울대 박사과정이 진행한 2017 패스트캠퍼스 강의와 코드를 참고하였음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 기법 개요

Jin&Tanaka(2006)의 아이디어는 생각보다 간단합니다. 단어 내부에서는 **불확실성(uncertainty)**, **엔트로피(entropy)**가 줄어들고, 경계에서는 증가하는 현상을 모델링한 것입니다. 아래 그림을 먼저 볼까요?

<a href="http://imgur.com/bv6xleH"><img src="http://i.imgur.com/bv6xleH.png" width="500px" title="source: imgur.com" /></a>

알파벳 'n' 한 글자만 주어졌을 땐 이 정보만으로는 어떤 단어가 등장할지 정확히 알기 어렵습니다(불확실성이 높은 상태). 하지만 글자가 'natur'까지 등장했다면 'nature'이거나 'natural' 두 가지 경우의 수뿐입니다(불확실성이 낮은 상태). 그럼 'nature' 다음에 나오는 글자는 무엇일까요? 다시 예측하기 어려워집니다. 이는 'natural'도 마찬가지입니다.

이미 주어진 글자정보를 활용해 다음 글자의 불확실성을 계산해보면 아래와 같은 그래프를 그릴 수 있습니다. 단어 내부에선 불확실성이 줄어들다가 단어 경계에서 불확실성이 다시 증가하기 때문입니다. Jin&Tanaka(2006)는 이런 점에 착안해 그 경계의 불확실성이 높은 글자들의 나열을 '단어'로 보자고 제안했습니다. 이때 쓰이는 불확실성 관련 지표가 바로 BE입니다.



<a href="http://imgur.com/FcOIZfz"><img src="http://i.imgur.com/FcOIZfz.png" width="500px" title="source: imgur.com" /></a>



BE는 아래와 같이 정의됩니다.


$$
H(X|{ X }_{ n })=-\sum _{ x\in X }^{  }{ P(x|{ x }_{ n })\times \log { (P(x|{ x }_{ n })) }  }
$$


## 분석 예시
영화 리뷰 사이트 '왓챠'에서 655만306개의 리뷰를 수집했습니다. 우선 이를 글자 단위로 세었습니다. 전체 결과 가운데 세 글자로 이뤄진 문자열 '아직까'를 포함하는 모든 단어들의 빈도는 아래 표와 같습니다. 우리는 아래 표로부터 '아직까'의 출현빈도는 5349라는 사실 또한 알 수 있습니다.



|   구분   |   빈도수    |
| :----: | :------: |
|  아직까지  |   4714   |
|  아직까진  |   632    |
|  아직까니  |    3     |
| **총합** | **5349** |



그렇다면 '아직까'의 엔트로피는 어떻게 구할까요? 아래와 같습니다.

<br>
$$
\begin{align*}
H(X|아직까)=&-P(아직까지|아직까)\times \log { (P(아직까지|아직까)) } \\&-P(아직까진|아직까)\times \log { (P(아직까진|아직까)) } \\&-P(아직까니|아직까)\times \log { (P(아직까니|아직까)) }\\\\=&-\frac { 4714 }{ 5349 } \times\log { \frac { 4714 }{ 5349 }  } -\frac { 632 }{ 5349 } \times\log { \frac { 632 }{ 5349 }  } -\frac { 3 }{ 5349 } \times\log { \frac { 3 }{ 5349 }  } \\\\ =&0.3679
\end{align*}
$$

위와 동일한 방식으로 '아직', '아직까지', '아직까지도'의 BE를 구해 비교하면 아래 표와 같습니다. 



|  구분  |  아직  | 아직까  | 아직까지 | 아직까지도 |
| :--: | :--: | :--: | :--: | :---: |
|  BE  | 2.95 | 0.37 | 3.46 | 4.69  |



위 표를 해석하면 이렇습니다. 우리가 온전한 단어로 쓰는 '아직', '아직까지', '아직까지도'는 그 경계에 다양한 글자들이 올 수 있으므로 BE가 비교적 높습니다. 하지만 '아직까'라는 문자열은 '아직까지', '아직까지도'라는 단어의 내부에 있으므로 BE가 낮습니다. 바꿔 말해 BE가 높은 문자열을 단어 취급해도 크게 나쁘지 않은 결과를 낼 수 있다는 것입니다.




## BE와 계열관계 

형태소란 **의미를 지니는 최소 단위**인데요, 형태소를 분석하는 기준으로는 **계열관계(系列關係paradigmatic relation)**가 있습니다. 계열관계는 그 자리에 다른 형태소가 ‘대치’될 수 있는가를 따지는 것입니다. 자세한 내용은 [이곳](https://ratsgo.github.io/korean%20linguistics/2017/03/20/morpheme/)을 참고하시면 좋을 것 같습니다.

어쨌든 BE는 사실상 **형태소(morpheme)** 추출 기법입니다. 말뭉치에서 어떤 문자열이 자주 쓰이는지 빈도를 세어 문자열마다 형태소가 될 만한 지표를 반환(엔트로피가 높을 수록 형태소일 확률이 큼)해 주는데요. 이는 BE가 형태소의 중요 분석기준인 계열관계와 밀접한 관련을 맺고 있기 때문입니다. 아래 그림을 볼까요?



<a href="http://imgur.com/of6pN5D"><img src="http://i.imgur.com/of6pN5D.png" width="300px" title="source: imgur.com" /></a>



위 예시에서 '엔진'이라는 명사 뒤에는 조사 '-이', '-에서', '-을' 등이 올 수 있습니다. 여기에서 '-이', '-에서', '-을'은 대치해서 쓸 수 있기 때문에 계열관계를 이룬다고 말할 수 있습니다. 그런데 이 경우 '엔진'의 BE는 매우 높을 겁니다. '엔진' 뒤에 다양한 문자열이 등장할 수 있어 불확실성이 크기 때문입니다. 

지금까지는 왼쪽에서 오른쪽으로 빈도수를 세는 걸 기준으로 설명을 해드렸지만, 오른쪽에서 왼쪽으로 빈도를 세어 BE를 계산하는 경우에도 마찬가지입니다. 아래 그림에서 '엔진'은 'Bracket', 'Head'와 계열관계를 맺고 있습니다. 아울러 '-에서'의 BE 또한 높습니다.



<a href="http://imgur.com/NGPpdVr"><img src="http://i.imgur.com/NGPpdVr.png" width="300px" title="source: imgur.com" /></a>



## 말뭉치 분석 결과

왓챠 리뷰 655만306개 리뷰를 학습해 BE 상위 500개 문자열을 나열한 결과는 아래와 같습니다. (left-side BE * right-side BE 결과를 내림차순 정렬)

<p class="message">
만큼이나, 혹은, 처럼, 이나, 하고, 또는, 하며, 하거나, 하면서도, 덕분에, 이랑, 덕에, 대신, 속에서, 앞에서, 함과, 때문에, 에서의, 속의, 그리고, 했고, 보다는, 등등, 에서, 이라는, 또한, 이자, 하고도, 하다가, 땜에, 이든, 할때, 속에서도, 해서, 하던, 때문인지, 없이, 하게, 이라도, 위에서, 사이의, 마저도, 마저, 안에서, 조차, 조차도, 하나로, 하지만, 때매, 보단, 아저씨가, 에다가, 이지만, 했으며, 이후로, 같이, 인데, 같은, 스의, 마냥, 하는, 둘다, 감독은, 이며, 같은거, 함은, 들이, 아저씨의, 감독의, 감독이, 만큼은, 아래, 등의, 없이도, 만큼, 스가, 함이, 할만큼, 속으로, 해도, 으로, 이란, 함에, 했던, 거기에, 보다, 에서는, 에는, 했지만, 스와, 스는, 에서부터, 했다가, 들을, 세계의, 까지, 함으로, 에선, 에게서, 함을, 뒤에서, 감독님의, 들은, 씬에서, 이라며, 오빠, 위의, 만큼의, 만이, 속에서의, 시키고, 존나, 형의, 치고, 이라서, 하면서, 시가, 인지, 씬은, 사이에서, 한테, 당하는, 시키는, 하나는, 영화답게, 아저씨, 이었는데, 에만, 위에, 해지고, 에서도, 했으나, 이를, 집에서, 버전의, 형님의, 맞고, 이와, 뒤에, 적인, 적이고, 했다면, 그가, 할땐, 나는, 둘이, 영화의, 진짜, 액션의, 적이며, 함의, 특유의, 스를, 이었지만, 다가, 사이로, 보다도, 함도, 하다가도, 하나, 언니, 한데, 이기에, 발로, 형님이, 이처럼, 감독님이, 다음으로, 씬과, 에의, 들과, 이라면, 배우는, 그의, 시를, 시리즈는, 너의, 이었고, 하는게, 하듯, 들도, 대는, 할정도로, 거나, 성과, 해보이는, 졸라, 이거나, 작가가, 만을, 성의, 앞에선, 그를, 네가, 으로만, 되며, 일지라도, 하나를, 그렇게, 그는, 캐릭터는, 일때, 이라지만, 탓에, 인과, 함으로써, 했는데, 하다보니, 이후의, 하는데, 겁나, 감독을, 누나, 하니, 따위는, 2는, 에게, 이전에, 좀더, 했을때, 위를, 작은, 배우님, 형이, 씬이, 혼자, 만의, 하느라, 사이를, 드라마의, 하면, 뒤의, 옆에서, 영화들의, 결론은, 빨리, 없이는, 같은게, 수준의, 까지도, 감독님, 성을, 하더니, 이후에, 이었다면, 역의, 심지어, 이야기는, 언니가, 자체가, 버전, 아저씨는, 이라니, 했기에, 이고, 이름이, 영화로, 해놓고, 안에서의, 스도, 말고는, 이전의, 형은, 만으로는, 그들이, 영화는, 근데, 연기의, 액션이, 이요, 한건지, 방에서, 아주, 인데도, 인은, 과의, 주인공은, 세대의, 주제에, 이던, 부터, 빼고는, 영화라면, 자는, 속을, 해버리는, 딸이, 그녀가, 광고, 모두가, 이어도, 하니까, 이가, 1의, 영화인데, 캐릭터가, 커플이, 그나저나, 들까지, 감독에게, 하려면, 거리며, 스랑, 언니의, 당하고, 정도의, 사건을, 정의, 관객들이, 영화도, 갑자기, 시대의, 주인공이, 영화계의, 나를, 삼아, 스토리를, 게다가, 그러나, 들로, 같은건, 영화치곤, 작가의, 아이가, 중의, 모두를, 여자는, 터지는, 으로부터, 이였지만, 으며, 가서, 마지막에, 이후, 할때는, 이라고, 한건, 스에게, 시간에, 그것을, 보다가, 다른, 영화로는, 적으로, 피가, 자신이, 얼굴이, 결말은, 할때마다, 저런, 씬의, 주의, 밑에서, 영화지만, 영화처럼, 차는, 그녀의, 스처럼, 이면, 암튼, 이라는게, 손으로, 머리를, 자신의, 그래도, 맨날, 고도, 하라는, 부터가, 사는, 다들, 영화들은, 오빠가, 여자가, 속은, 영화와, 그치만, 하듯이, 이영화는, 영화이자, 드라마, 모두, 하지말고, 수준으로, 한듯한, 인이, 그에게, 시리즈의, 관계의, 버젼, 등이, 나의, 간지, 보니까, 시리즈를, 급의, 감독님은, 그것이, 모두에게, 괜히, 부분은, 치고는, 했는지, 시리즈가, 영화에서, 이라던가, 이냐, 남의, 해온, 물에서, 영화치고는, 액션은, 관객이, 시와, 그와중에, 레알, 드와, 속에도, 없는, 치며, 그들의, 팀이, 카드, 피는, 형님, 관객들은, 그것은, 성은, 장은, 인건지, 연출은, 점은, 캐릭터의, 인도, 트는, 드의, 배우들이, 스타일로, 스토리가, 시리즈에서, 시간을, 말고, 했어도, 주인공들이, 되게, 집을, 아줌마, 캐릭터를, 수와, 하다못해, 너는, 했으니, 강한, 할아버지가, 이의, 선생님의, 나오는, 화된, 모든게, 보는데, 게이, 되서, 문화를, 아빠가, 손이, 하도록, 타는, 지를, 온갖, 등으로, 중간에, 엄마가, 거리는, 살이, 몸이, 팬들은, 관객은, 이번엔, 이야기가, 이니까, 남자는, 마치, 적은, 그들은, 속에는, 하였고, 프로, 씬을, 엔딩은, 경찰이, 들이나, 어설픈, 배우가, 남자가, 나올때마다, 저렇게, 다며, 시켜서, 이렇게, 애니메이션의
</p>



## 코드

김현중 박사과정이 작성한 BE 코드를 사용했습니다. 저 역시 정리 용도로 남긴 것이니 문제되면 바로 삭제하겠습니다. 최신 코드는 김현중 박사과정의 깃헙 https://github.com/lovit/soy을 참고하시기 바랍니다. 

사용법은 아래와 같습니다. 아래 코드에서 *MaxScoreTokenizer* 역시 김현중 박사과정이 만든 코드로 말뭉치에서 학습한 BE를 바탕으로 문장을 토큰으로 나눠주는 함수입니다.

```python
import branching_entropy as tool
branching = tool.BranchingEntropy()
branching.train(reviews)
branchingtokenizer = tool.MaxScoreTokenizer(scores=branching.get_all_branching_entropies())
branching_tokenized_reviews = [branchingtokenizer.tokenize(review) for review in reviews]
```



<script src="https://gist.github.com/ratsgo/9d67443a515f0fed62da66d647575d4b.js"></script>