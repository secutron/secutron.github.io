---
title: 자기조직화지도(Self-Organizing Map)
category: Machine Learning
tag: Clustering
---

이번 글에서는 **차원축소(dimensionality reduction)**와 **군집화(clustering)**를 동시에 수행하는 기법인 **자기조직화지도(Self-Organizing Map, SOM)**를 살펴보도록 하겠습니다. 이번 글 역시 고려대 강필성 교수님 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 아키텍처 개요

SOM이란 사람이 눈으로 볼 수 있는 저차원(2차원 내지 3차원) 격자에 고차원 데이터의 각 개체들이 대응하도록 인공신경망과 유사한 방식의 학습을 통해 군집을 도출해내는 기법입니다. 고차원의 데이터 원공간에서 유사한 개체들은 저차원에 인접한 격자들과 연결됩니다. 저차원 격자에서의 유사도는 고차원 입력 공간에서의 유사도를 최대한 보존하도록 학습됩니다. 

SOM 아키텍처의 핵심은 대략 아래 그림과 같습니다.

<a href="http://imgur.com/ZsAdHxT"><img src="http://i.imgur.com/ZsAdHxT.png" width="400px" title="source: imgur.com" /></a>

위 그림에서 초록색 노드($x_i$)는 $n$차원 입력벡터의 각 요소를 뜻합니다. 주황색 노드($w_j$)는 2차원 격자입니다. 

저차원 격자 하나에는 여러 개의 입력벡터들이 속할 수 있습니다. 여기에 속한 입력벡터들끼리는 서로 위치적인 유사도를 가집니다(=가까운 곳에 있음).

그럼 임의의 입력벡터가 주어졌을 때 2차원상 어떤 격자에 속하는지 어떻게 알 수 있을까요? 위 그림 기준으로 $j$번째 격자는 원데이터 공간에 존재하는 $n$차원 벡터 $[w_{j1},w_{j2},...,w_{jn}]$에 대응됩니다. 

다시 말해 2차원상 격자가 위 그림처럼 25개라면 그에 해당하는 $n$차원 크기의 격자벡터도 25개 있다는 이야기이죠.

임의의 $n$차원 입력벡터가 들어왔을 때 가장 가까운 격자벡터를 찾습니다. 이것을 **Winning node**라고 합니다. 이 벡터에 대응되는 2차원상 격자에 해당 입력벡터를 할당하면 이것이 바로 군집화가 되는 것입니다.

같은 격자에 할당된 입력벡터라 하더라도 Winning node와의 거리가 제각기 다를 수 있습니다. 이러한 멀고 가까움 또한 표시를 하게 되면 고차원 공간의 원데이터를 2차원 내지 3차원으로 차원을 축소하는 효과까지 낼 수 있습니다.

SOM의 결과물 예시는 아래 그림과 같습니다. 고차원 공간의 원데이터가 25개 격자에 각각 할당(군집화)돼 있고, 동일한 격자에 할당된 입력값끼리도 그 위치가 서로 다르게 임베딩된 것을 확인할 수 있습니다.

<a href="http://imgur.com/EE8NF6J"><img src="http://i.imgur.com/EE8NF6J.png" width="400px" title="source: imgur.com" /></a>



## SOM 학습

그렇다면 SOM은 어떻게 학습을 하는 것일까요? 직관적으로 이해해보기 위해 아래 그림을 먼저 살펴보겠습니다. 시각화를 위해 원데이터의 차원수와 격자의 차원수는 모두 2차원으로 두었습니다. 하지만 이는 이해를 돕기 위함일 뿐, 검정색 점이 흩뿌려진 원데이터 공간은 고차원이라고 생각하셔야 합니다.

<a href="http://imgur.com/eHUVAtr"><img src="http://i.imgur.com/eHUVAtr.gif" width="400px" title="source: imgur.com" /></a>

위 그림에서 연두색 25개 점은 고차원의 원데이터공간에 대응되는 격자벡터입니다. 처음엔 그 위치를 랜덤으로 초기화합니다. 이후 학습데이터 $x$(검정색 점)를 하나씩 추가해가며 격자벡터의 위치를 $x$의 위치와 비슷해지도록 업데이트합니다(**instance-based learning**). 

다만 여기서 주의해야할 것은 격자벡터의 업데이트 폭이 모두 같지는 않다는 점입니다. 현재 주어진 $x$와 그 위치가 가장 가까운 Winning node가 제일 많이 업데이트되고요, 이 노드의 주변 격자벡터들은 그보다 조금 덜 업데이트됩니다. 멀리 떨어진 녀석들은 거의 업데이트가 되지 않도록 합니다.

$t$시점의 $j$번째 격자벡터를 업데이트하는 과정을 수식으로 나타내면 아래와 같습니다. 



$${ w }_{ j }^{ t+1 }={ w }_{ j }^{ t }+{ \mu  }_{ t }{ \lambda  }_{ x }^{ j,t }[x-{ w }_{ j }^{ t }]$$



여기에서 $μ$는 일종의 learning rate로 반복 횟수가 커지면 학습 속도를 줄이는 역할을 합니다. λ는 Winning node는 가장 크게, 학습데이터와 멀리 떨어진 격자벡터는 작게 업데이트하도록 합니다(가우시안 분포 활용). 마지막으로 괄호 안의 뺄셈으로 된 부분은 학습데이터 $x$와 격자벡터 간 차이를 의미하는데, 벡터의 덧/뺄셈 결과 역시 벡터이므로 격자벡터가 업데이트되는 **방향**을 결정해줍니다.