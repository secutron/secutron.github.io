---
title: supervised generative models 
category: generative model
tag: [naive bayes, LDA]
---

이번 글에서는 지도학습(supervised learning) 기반의 **generative model**을 [나이브 베이즈 모델](https://ratsgo.github.io/machine%20learning/2017/05/18/naive/), [선형판별분석 모델](https://ratsgo.github.io/machine%20learning/2017/03/21/LDA/) 중심으로 살펴보도록 하겠습니다. 이 글은 전인수 서울대 박사과정이 2017년 12월에 진행한 패스트캠퍼스 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.





## generative model

*generative model*은 데이터 $x$가 생성되는 과정을 두 개의 확률모형, 즉 $p(y)$와 $p(x$\|$y)$로 정의합니다. 지도학습 기반의 *generative model*은 레이블 $y$가 명시적으로 주어진 경우를 가리킵니다. 대표적으로 나이브 베이즈 모델(naive bayes models)과 선형판별분석(linear discriminant analysis)이 있습니다.





## 나이브 베이즈 모델

나이브 베이즈 모델을 *generative modeling* 관점에서 분석하면 다음과 같습니다.


$$
p\left( y \right) \sim { \phi  }_{ y }\\ p\left( { x }_{ j }|y=0 \right) \sim { \phi  }_{ j|y=0 }\\ p\left( { x }_{ j }|y=1 \right) \sim { \phi  }_{ j|y=0 }
$$


여기에서 $x$는 데이터, $y$는 범주를 나타냅니다. 영화리뷰를 대상으로 나이브 베이즈 모델을 구축한다면 $x$는 리뷰, $y$는 긍정(1), 부정(0) 극성이 될 겁니다. $y$는 파라메터 $Φ_y$를 가지는 [베르누이분포](https://ratsgo.github.io/statistics/2017/09/21/prob/)(성공 또는 실패 두 가지 가능한 결과에 대한 확률분포)를 따른다고 가정합니다.

$x_j$는 영화 리뷰에서 $j$번째 단어를 가리킵니다. 영화 리뷰가 부정($y=0$)일 때 $j$번째 단어가 등장할 확률 $p(x_j$\|$y=0)$은 파라메터 $Φ_{j,y=0}$을 가지는 베이누이분포를 따른다고 가정합니다. 다시 말해 이 확률은 $j$번째 단어가 한번 이상 나타날 가능성을 가리킵니다. 

나이브베이즈 모델의 우도 함수는 다음과 같습니다. $x$와 $y$의 결합확률(joint probability)을 베이즈룰(bayes rule)에 따라 다시 적은 겁니다.


$$
\begin{align*}
L({ \phi  }_{ y },{ \phi  }_{ k|y=0 },{ \phi  }_{ k|y=1 })=&\prod _{ i=1 }^{ m }{ p\left( { x }^{ (i) },{ y }^{ (i) } \right)  } \\ =&\prod _{ i=1 }^{ m }{ \left( \prod _{ j=1 }^{ { n }_{ i } }{ p\left( { x }_{ j }^{ (i) }|y;{ \phi  }_{ k|y=0 },{ \phi  }_{ k|y=1 } \right)  }  \right) p\left( { y }^{ (i) };{ \phi  }_{ y } \right)  } 
\end{align*}
$$


위 식에 로그를 취하고 각각의 파라메터에 대해 미분을 하여 0이 되는 지점을 찾는 방식으로 계산하면 각각의 파라메터는 다음과 같이 분석적으로 도출할 수 있습니다. 전체 데이터(말뭉치)를 대상으로 아래처럼 각 단어별로 빈도를 세면 파라메터 추정이 종료된 겁니다.


$$
{ \phi  }_{ y }=\frac { \sum _{ i }^{  }{ I\left\{ { y }^{ (i) }=1 \right\}  }  }{ m } \\ { \phi  }_{ k|y=0 }=\frac { \sum _{ i }^{  }{ I\left\{ { y }^{ (i) }=0,{ x }_{ j }^{ (i) }=1 \right\}  }  }{ \sum _{ i }^{  }{ I\left\{ { y }^{ (i) }=0 \right\}  }  } \\ { \phi  }_{ k|y=1 }=\frac { \sum _{ i }^{  }{ I\left\{ { y }^{ (i) }=1,{ x }_{ j }^{ (i) }=1 \right\}  }  }{ \sum _{ i }^{  }{ I\left\{ { y }^{ (i) }=1 \right\}  }  } 
$$


이렇게 추정된 파라메터를 가지고 새로운 데이터 $x$에 대한 레이블 추정은 다음과 같이 베이즈룰을 이용합니다. 쉽게 말해 우도 $p(x$\|$y)$와 사전확률 $p(y)$ 곱이 큰 쪽으로 $y$를 추정하는 겁니다.


$$
\begin{align*}
arg\max _{ y }{ p(y|x) } =&arg\max _{ y }{ \frac { p(x|y)p\left( y \right)  }{ p\left( x \right)  }  } \\ =&arg\max _{ y }{ p(x|y)p\left( y \right)  } 
\end{align*}
$$




## 선형판별분석

선형판별분석을 *generative modeling* 관점에서 분석하면 다음과 같습니다. 다시 말해 $p(x$\|$y)$가 정규분포를 따르고, 각각의 분포는 분산이 $Σ$로 같다고 가정합니다. 아울러 $y$는 파라메터가 $Φ$인 베르누이분포를 따른다고 가정합니다.


$$
p\left( y \right) \sim { \phi  }_{ y }\\ p\left( x|y=0 \right) \sim N\left( { \mu  }_{ 0 },\Sigma  \right) \\ p\left( x|y=1 \right) \sim N\left( { \mu  }_{ 1 },\Sigma  \right)
$$

각각의 확률함수는 다음과 같습니다.



$$
p\left( y \right) ={ { \phi  } }^{ y }{ { \left( 1-\phi  \right)  } }^{ y }\\ p\left( x|y=0 \right) =\frac { 1 }{ { \left( 2\pi  \right)  }^{ n/2 }{ \left| \Sigma  \right|  }^{ 1/2 } } exp\left( -\frac { 1 }{ 2 } { \left( x-{ \mu  }_{ 0 } \right)  }^{ T }{ \Sigma  }^{ -1 }{ \left( x-{ \mu  }_{ 0 } \right)  } \right) \\ p\left( x|y=1 \right) =\frac { 1 }{ { \left( 2\pi  \right)  }^{ n/2 }{ \left| \Sigma  \right|  }^{ 1/2 } } exp\left( -\frac { 1 }{ 2 } { \left( x-{ \mu  }_{ 1 } \right)  }^{ T }{ \Sigma  }^{ -1 }{ \left( x-{ \mu  }_{ 1 } \right)  } \right)
$$

선형판별분석 모델의 로그우도 함수는 다음과 같습니다. $x$와 $y$의 결합확률(joint probability)을 베이즈룰(bayes rule)에 따라 다시 적은 겁니다.



$$
\begin{align*}
l(\phi ,{ \mu  }_{ 0 },{ \mu  }_{ 1 },\Sigma )=&\log { \prod _{ i=1 }^{ m }{ p\left( { x }^{ (i) },{ y }^{ (i) };\phi ,{ \mu  }_{ 0 },{ \mu  }_{ 1 },\Sigma  \right)  }  } \\ =&\log { \prod _{ i=1 }^{ m }{ p\left( { x }^{ (i) }|{ y }^{ (i) };\phi ,{ \mu  }_{ 0 },{ \mu  }_{ 1 },\Sigma  \right) \cdot p\left( { y }^{ (i) };{ \phi  } \right)  }  } 
\end{align*}
$$



선형판별분석 모델의 파라메터는 위 로그 우도함수를 각각의 파라메터에 대해 미분해서 0이 되는 지점을 찾는 방식으로 분석적으로 도출할 수 있습니다. 자세한 도출 과정은 [이곳](https://towardsdatascience.com/gaussian-discriminant-analysis-an-example-of-generative-learning-algorithms-2e336ba7aa5c)을 참고하시면 좋을 것 같습니다. 

어쨌든 이렇게 추정된 파라메터를 가지고 새로운 데이터 $x$에 대한 레이블 추정은 다음과 같이 베이즈룰을 이용합니다. 쉽게 말해 우도 $p(x$\|$y)$와 사전확률 $p(y)$ 곱이 큰 쪽으로 $y$를 추정하는 겁니다.


$$
\begin{align*}
arg\max _{ y }{ p(y|x) } =&arg\max _{ y }{ \frac { p(x|y)p\left( y \right)  }{ p\left( x \right)  }  } \\ =&arg\max _{ y }{ p(x|y)p\left( y \right)  } 
\end{align*}
$$


